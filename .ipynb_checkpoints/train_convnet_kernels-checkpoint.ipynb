{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# from keras.callbacks import Callback, TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from convnet_sigma_err import *\n",
    "# from keras.utils import plot_model\n",
    "import random\n",
    "from data_generator_sigma_err import *\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy.ma as ma\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from plotting import results_categorical_histo #plot_emiss_signal, data_err_histo, \n",
    "# from gp_real_data import compute_abs_error\n",
    "import time\n",
    "# from postprocess import post_training\n",
    "mpl.use('Agg')\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    " \n",
    "def main(args):\n",
    "    exp_id = './exps/' + args[1]\n",
    "    train_dir = './exps/' + args[1]\n",
    "    if not os.path.isdir(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    print('Will save this model to', train_dir)\n",
    "    \n",
    "    checkpoint_dir = train_dir +'/model_checkpoints/'\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    logs_dir = train_dir +'/logs/'\n",
    "    if not os.path.isdir(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "        \n",
    "    hp_dic = load_dic(exp_id + '/hp_params')\n",
    "    sigma_fs = hp_dic['sigma_fs']\n",
    "    sigma_xs = hp_dic['sigma_xs']\n",
    "    sigma_errs = hp_dic['sigma_errs']   \n",
    "        \n",
    "    epoch_size = 128 #128\n",
    "    no_epocs = 5\n",
    "    no_sensors = hp_dic['measurement_dim']\n",
    "    # print(no_sensors)\n",
    "    # exit(0)\n",
    "    num_classes = len(sigma_fs)*len(sigma_xs)*len(sigma_errs)\n",
    "    bsize = num_classes*8 #/32/ samples per hyperparameter\n",
    "    \n",
    "    ensemble_size = 2\n",
    "    \n",
    "    def top_2_accuracy(y_true, y_pred):\n",
    "        return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
    "    \n",
    "    def top_3_accuracy(y_true, y_pred):\n",
    "        return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "    # -------- Training --------\n",
    "    params_random = {'dim': no_sensors,\n",
    "                      'no_batches_ep': epoch_size, \n",
    "                      'shuffle': True,\n",
    "                      'exp_id': exp_id,\n",
    "                      'batch_size': bsize,\n",
    "                      'epoch_size':epoch_size,\n",
    "                      # 'shots': train_val_shots,\n",
    "                      'n_splits':ensemble_size\n",
    "                      }\n",
    "    \n",
    "    print('experiment parameters', params_random)\n",
    "    \n",
    "    mainGenerator = DataGenerator(**params_random)\n",
    "    \n",
    "    ensemble = []\n",
    "    pred_hps_top_all = []\n",
    "    scores_matrix = {'train':np.empty((ensemble_size, len(sigma_fs)*len(sigma_errs)*len(sigma_xs))),\n",
    "                     'test':np.empty((ensemble_size, len(sigma_fs)*len(sigma_errs)*len(sigma_xs)))}\n",
    "    val_ms = []\n",
    "    val_errs = []\n",
    "    used_ms_all = []\n",
    "    st_all = []\n",
    "    # pred_deltas = []\n",
    "    \n",
    "    for k_fold in range(mainGenerator.n_splits):\n",
    "        print('----------------------------------------------Training member', k_fold+1, 'of ensemble...----------------------------------------------')\n",
    "        cnn = model(no_sensors, len(sigma_fs), len(sigma_xs), len(sigma_errs), k_fold)\n",
    "    \n",
    "    \n",
    "        cnn.compile(loss={'single_mclass':'categorical_crossentropy'},\n",
    "                    optimizer='adam',\n",
    "                    metrics={'single_mclass':['categorical_accuracy', top_2_accuracy, top_3_accuracy]})\n",
    "        \n",
    "        train_generator = TrainDataGenerator(k_fold, mainGenerator)\n",
    "        train_gen = next(iter(train_generator))\n",
    "        val_generator = ValDataGenerator(k_fold, mainGenerator)\n",
    "        val_gen = next(iter(val_generator))\n",
    "        # mainGenerator.get_stats(), train_generator.get_stats(), val_generator.get_stats()\n",
    "        # exit(0)\n",
    "        train_start = time.time()\n",
    "        cnn.fit_generator(generator = train_gen, steps_per_epoch=epoch_size, epochs=no_epocs, validation_data=val_gen, validation_steps=epoch_size)#, callbacks=[saveCheckpoint,]) #,tb ,, validation_steps=bsize\n",
    "        train_end = time.time()\n",
    "        print('----------------------------------------------')\n",
    "        print('training took:', train_end-train_start, 'seconds')\n",
    "        print('----------------------------------------------')\n",
    "    \n",
    "        scores = get_scores(train_generator, cnn, num_classes)\n",
    "        scores_matrix[str(train_generator)][k_fold] = np.asarray(scores)\n",
    "    \n",
    "        scores = get_scores(val_generator, cnn, num_classes)\n",
    "        scores_matrix[str(val_generator)][k_fold] = np.asarray(scores)\n",
    "        \n",
    "        \n",
    "    # exit(0)\n",
    "    # print('----------------------------------------------')\n",
    "    # print('average fraction of used measurements per data point:', np.mean(used_ms_all))\n",
    "    # print('----------------------------------------------')\n",
    "    # print('time the classifiers took to predict across their k-fold split:', np.asarray(pred_deltas))\n",
    "    # print('average time the classifiers took to predict across their k-fold split:', np.mean(pred_deltas))\n",
    "    # print('total time the classifiers took to predict across the entire k-fold split:', np.sum(pred_deltas))\n",
    "    # print('----------------------------------------------')\n",
    "    \n",
    "    np.save(exp_id + '/scores_matrix_val', scores_matrix[str(val_generator)])\n",
    "    np.save(exp_id + '/scores_matrix_train', scores_matrix[str(train_generator)])\n",
    "    # print(scores_matrix[str(val_generator)])\n",
    "    # print(scores_matrix[str(val_generator)].shape)\n",
    "    # print(np.mean(scores_matrix[str(val_generator)], axis=0))\n",
    "    # print(np.var(scores_matrix[str(val_generator)], axis=0))\n",
    "    pdf_handler = PdfPages(exp_id + '/accuracy_histogram_val.pdf')\n",
    "    results_categorical_histo(pdf_handler, scores_matrix[str(val_generator)])\n",
    "    pdf_handler = PdfPages(exp_id + '/accuracy_histogram_train.pdf')\n",
    "    results_categorical_histo(pdf_handler, scores_matrix[str(train_generator)])\n",
    "\n",
    "def get_scores(generator, cnn, num_classes):\n",
    "    inputs, targets = generator.get_all_items()\n",
    "    measurements = inputs\n",
    "    labels = targets#[:10]\n",
    "    labels_argmax = np.argmax(labels, axis=1)\n",
    "    \n",
    "    print('classifier predicting on its val data, whose shape is', measurements.shape, labels.shape)#, train_val_hps[:100].shape) errs.shape,  labels_argmax.shape\n",
    "    # pred_start = time.time()\n",
    "    pred_hps = cnn.predict(measurements)\n",
    "    # pred_end = time.time()\n",
    "    # pred_delta = pred_end-pred_start\n",
    "    # print('----------------------------------------------')\n",
    "    # print('prediction took:', pred_delta, 'seconds')\n",
    "    # pred_deltas.append(pred_delta)\n",
    "    # print('----------------------------------------------')\n",
    "    \n",
    "    argsort_preds = np.argsort(pred_hps, axis=1)\n",
    "    # pred_hps_top_all.extend(argsort_preds[:, -1])\n",
    "    print('shape of predictions of whole dataset, ', pred_hps.shape)\n",
    "    acc_sum = 0\n",
    "    acc_list = []\n",
    "    acc_h = []\n",
    "    for k in range(num_classes):\n",
    "        acc_int = accuracy_score(labels_argmax, argsort_preds[:,-1-k], normalize=False)\n",
    "        acc_sum += acc_int\n",
    "        acc_h.append(acc_sum/len(labels_argmax))\n",
    "        acc_list.extend(np.ones(acc_sum) * k)\n",
    "    return acc_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save this model to ./exps/exp5\n",
      "experiment parameters {'dim': 136, 'no_batches_ep': 128, 'shuffle': True, 'exp_id': './exps/exp5', 'batch_size': 144, 'epoch_size': 128, 'n_splits': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frdmat/Documents/tomography/plotting.py:50: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10000  data  samples. Hyperparameters were: f-> [ 10  75 250] x [0.125 0.175 0.225] err [0.5 1. ]\n",
      "------------------Printing stats for main generator------------------\n",
      "f,x,err,count\n",
      "0-0-0 174\n",
      "0-0-1 1043\n",
      "0-1-0 201\n",
      "0-1-1 407\n",
      "0-2-0 363\n",
      "0-2-1 639\n",
      "1-0-0 728\n",
      "1-0-1 1015\n",
      "1-1-0 710\n",
      "1-1-1 1043\n",
      "1-2-0 593\n",
      "1-2-1 1010\n",
      "2-0-0 318\n",
      "2-0-1 28\n",
      "2-1-0 649\n",
      "2-1-1 115\n",
      "2-2-0 644\n",
      "2-2-1 320\n",
      "Total number of samples: 10000\n",
      "----------------------------------------------Training member 1 of ensemble...----------------------------------------------\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "m (InputLayer)               [(None, 136, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 136, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 136, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 136, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 68, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 68, 64)            6208      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 68, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 68, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 68, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 34, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 34, 128)           24704     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 34, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 34, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 34, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 17, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 17, 256)           98560     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 17, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 17, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 17, 256)           1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "single_mclass (Dense)        (None, 18)                9234      \n",
      "=================================================================\n",
      "Total params: 10,121,074\n",
      "Trainable params: 10,110,898\n",
      "Non-trainable params: 10,176\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-1-a409b1021bc5>:100: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/5\n",
      "128/128 [==============================] - 81s 633ms/step - loss: 1.6135 - categorical_accuracy: 0.4547 - top_2_accuracy: 0.6949 - top_3_accuracy: 0.8219 - val_loss: 6.4973 - val_categorical_accuracy: 0.0678 - val_top_2_accuracy: 0.1368 - val_top_3_accuracy: 0.2039\n",
      "Epoch 2/5\n",
      "128/128 [==============================] - 87s 683ms/step - loss: 0.9160 - categorical_accuracy: 0.6472 - top_2_accuracy: 0.8746 - top_3_accuracy: 0.9518 - val_loss: 5.2613 - val_categorical_accuracy: 0.1085 - val_top_2_accuracy: 0.2164 - val_top_3_accuracy: 0.3616\n",
      "Epoch 3/5\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.6235 - categorical_accuracy: 0.7527 - top_2_accuracy: 0.9381 - top_3_accuracy: 0.9813"
     ]
    }
   ],
   "source": [
    "main(['', 'exp5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
